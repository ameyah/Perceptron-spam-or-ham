Assignment 2 Report

Name: Ameya Hanamsagar
Email: ahanamsa@usc.edu

Part I.

1. Performance of standard perceptron on the development data with 100% of the training data
1a. spam precision: 0.9866557734204793
1b. spam recall: 0.9858503401360544
1c. spam F1 score: 0.9862528923370084
1d. ham precision: 0.9654025282767797
1e. ham recall: 0.9673333333333334
1f. ham F1 score: 0.9663669663669664

2. Performance of averaged perceptron on the development data with 100% of the training data
2a. spam precision: 0.9858464888405009
2b. spam recall: 0.985578231292517
2c. spam F1 score: 0.985712341815213
2d. ham precision: 0.9646902065289806
2e. ham recall: 0.9653333333333334
2f. ham F1 score: 0.9650116627790736

Part II.

3. Performance of standard perceptron on the development data with 10% of the training data
3a. spam precision: 0.9811163565676201
3b. spam recall: 0.9613605442176871
3c. spam F1 score: 0.9711379879054426
3d. ham precision: 0.9097839898348158
3e. ham recall: 0.9546666666666667
3f. ham F1 score: 0.9316851008458035

4. Performance of averaged perceptron on the development data with 10% of the training data
4a. spam precision: 0.9782967032967033
4b. spam recall: 0.9689795918367347
4c. spam F1 score: 0.973615857826384
4d. ham precision: 0.9257328990228013
4e. ham recall: 0.9473333333333334
4f. ham F1 score: 0.9364085667215816

Part III. You are welcome to reuse code you wrote for assignment 1,
but we would like to know how you handled the following tasks.

5. How did you calculate precision, recall and F1 score? If you used a
separate script, please give the name of the script and describe how
to run it.

    First, I calculated the necessary data (counters) required for calculation of precision, recall
    and F1 score.
    Note: I chose spam label as -1 and ham label as 1 (value of y in algorithm)
    # of documents classified as <class>: I just kept a counter and incremented it every time a
    document is classified as <class> (spam/ham) (if activation > 0, then its ham, otherwise spam)
    # of documents correctly classified as in <class>: Incremented a counter for document classified
    as <class> and checked whether that file contains the name of <class>
    # of documents belonging to <class>: Incremented a counter every time a file belonging to <class>
    is encountered, determined by whether the name <class> is in file name.

    Precision, Recall and F1 score are calculated using the formulae given in Naive Bayes slides
    precision = # of documents correctly classified as in <class> / # documents classified as <class>
    recall = # of documents correctly classified as in <class> / # documents belonging to <class>
    F1 score of <class> = 2 * precision * recall / (precision + recall)

    Precision, Recall and F1 scores are echoed to standard output when the mode (variable in per_classify.py)
    is changed to "DEV"


6. How did you separate 10% of the training data? If you used a
separate script, please give the name of the script and describe how
to run it. Explain how you or your code choose the files.

    The program automatically chooses 10% of the files when provided an additional command-line
    argument "-l 10" to the per_learn.py or avg_per_learn.py (Here, 10 means we want to consider
    10% of the training data)
    For example, python3 per_learn.py train_dir/ -l 10

    When supplied with the '-l' flag, the program first calculates the total number of spam and
    ham files, and then calculates 10% of the spam/ham files.

    The code chooses files only from the directory 'train/2' which is handled programmatically and
    is chosen in the order which Ubuntu chose during os.walk. After each spam/ham file is detected
    during another iteration of os.walk, the 10% file count is decreased by 1 until it becomes 0.
    This way we know that we have 10% files from both the categories.